%! Author = emaia
%! Date = 14/03/20

In this section we briefly introduce the concept of genetic algorithms (GA) in Search-Based Software Engineering (SBSE) and some well-known implementations in literature.

% SBSE
SBSE focuses on using search algorithms and techniques to solve Software Engineering-related problems~\cite{anand2013jss_survey};
in particular, Search-Based Software Testing (SBST) makes an extensive use of these techniques for solving problems
like Test Case Generation, Selection or Prioritization.
One of the most prominent problem in software testing is providing a minimal-cost test suite that maximizes a certain
criterion (\textit{e.g.,} covering the largest number of branches).
Generating test cases (sometimes only their input) means using search algorithms guided by a
\textit{fitness function} that models a set of \textit{test goals} (\textit{a.k.a.,} test objectives).
The fitness function guides the search in the space of test cases to find those that best meet the test
goals;
this approach is highly generic and widely applicable is various forms.
There are many different kinds of search techniques, but in recent years the literature has been working on evolutionary approaches, in particular GAs.

\subsection{Genetic Algorithms}\label{subsec:genetic}
% GA: meta-heuristic
A GA is a particular type of a meta-heuristic optimization algorithm, \textit{i.e.,} a general technique that can be applied to a broad range of search problems.
It starts with an initial random \textit{population} of \textit{individuals} (\textit{a.k.a.,} chromosomes),
which are candidate solutions for the given problem and whose genetic structure depends on the specific problem representation.
The population starts to evolve by passing through different iterations (generations) that continuously change its individuals
in order to produce the best solutions;
in a single iteration each individual is given to the fitness function that produces a score that represents its chance of surviving in the evolution.
The evolution consists of applying various operators on individuals, such as (i) \textit{selection}, that choose
the best individuals (the higher the fitness score, the higher the probability to be chosen) for the creation of the next generation of individuals;
(ii) \textit{crossover}, that applies some \textit{genes} interchange to each pair of selected \textit{parents} to generate their \textit{offsprings},
that will be added in the next generation;
(iii) \textit{mutation}, that randomly changes some genes of the newly-formed offsprings with a certain probability.
There may be different termination criteria: (i) the search budget (\textit{e.g.,} time) expires;
(ii) the population reaches the convergence (\textit{i.e.,} it has an individual with perfect fitness) or
(iii) the algorithm makes no progresses, meaning that for some iterations the aggregate fitness score (related to the entire population) made no improvements.

% GA: Single vs Many
Typically, a fitness function is expressed in terms of a min function (\textit{i.e.,} the optimal value is the minimum).
While common implementations use a single fitness function (single-objective), others prefers using multiple, often
conflicting, functions (many-objective), implying a redefinition of the concept of optimality that takes into
account multiple fitness scores --- collected in a \textit{fitness vector} --- and the trade-offs.
In many-objective contexts we use the broader concept of \textit{Pareto optimality}~\cite{deb2005moo}, meaning that an individual is better
than another one when it \textit{dominates} it, \textit{i.e.,} when it has at least one better fitness score than the other
individual while the other scores are not worse than the other's. \textbf{I don't like this sentence, should I use some formulas?
DynaMosa paper has some}

% GA: in SBST
In Test Case Generation, a test suite is modeled as a population of evolving test cases (individuals), which are
composed of a list of statements (genes).
The test goals, that will define the fitness function(s), depend on the chosen \textit{coverage criteria}, that is commonly a
coverage measure from white-box testing, such as branch, line or mutation coverage.
The fitness function(s) will be based on how much the execution trace of a test case is distant from the aforementioned goals;
for instance, for branch coverage the distance is based on the number of control dependencies that separate the execution trace
from the target branch (\textit{approach level}) and on the variable values evaluated at the conditional expression where the execution
diverges from the target (\textit{branch distance})~\cite{panichella2018tse_dynamosa}.

\subsection{Notable Examples}\label{subsec:examples}
The various forms of fitness functions are not the only variants that can be applied on GAs;
indeed, in literature there are numerous examples of how malleable a GA is.

% MOSA: preference and archive
\citeauthor{panichella2015icst_mosa}~\cite{panichella2015icst_mosa} proposed MOSA (Many-Objective Sorting Algorithm),
which uses multiple fitness functions, one for each branch of the SUT (any other coverage criterion is valid),
to generate the best set of non-dominated test cases that minimize all the fitness functions, \textit{a.k.a.,} the \textit{Pareto front}.
In many-objective problems, however, the number of non-dominated individuals increases exponentially with the number of objectives,
making the search much more difficult (\textit{i.e.,} the convergence is hardly reached) and the algorithm performs like a random search one.
Thus, domain specific knowledge is needed to impose some \textit{preference criteria} among non-dominated test cases;
for test case generation, this means preferring test cases that (i) cover (or are close to cover) uncovered targets and
(ii) are smaller than other competitors;
so, for each test goal, the best individual is the closest and smallest one.
As consequence, the set of candidate (for reproduction) individuals is a subset of Pareto front.
%, \textit{i.e.,} the set of all best test cases.
In addition to these preference criteria, MOSA adds the \textit{archive} technique, that consists of keeping a second
non-evolving population of test cases.
Specifically, whenever new test cases are generated MOSA stores in the archive those individuals that satisfy previously
uncovered targets;
after the final iteration, the test cases are picked from both the last population and the archive to form the final test suite.

% DynaMOSA: dynamic selection of targets
One main limitation of MOSA is that it considers all coverage goals as independent objectives;
however, there exist structural dependencies among targets that should be considered when deciding
which one to optimise;
for example, some branches can be satisfied if all its control dependency holders (branches) are already covered.
To overcome this limitation, \citeauthor{panichella2018tse_dynamosa}~\cite{panichella2018tse_dynamosa} proposed DynaMOSA (Dynamic MOSA)
that is able, in each iteration, to \textit{dynamically select targets} whose control dependency holders have already been covered in previous
iterations while ignoring the other targets.
This idea makes the search more effective and efficient in situations where there are a lot of dependencies between coverage goals.

% MIO: independent evolutionary algorithm
A slightly different approach is with MIO (Many Independent Objectives)~\cite{arcuri2017lncs_mio}, an evolutionary algorithm
that works well for hundreds/thousands of independent (though not incompatible) goals.
It keeps one population for each goal (\textit{e.g.}, method) --- called \textit{islands} --- to evolve independently from the others
(the individuals of an island are only evaluated against the respective fitness function).
The reproduction mechanism is not based on genetic operators (this is why MIO is not classified as a GA), but rather on sampling individuals either
randomly or from other islands (\textit{i.e., migration}).
Whenever a goal is covered, its island stops the evolution and only the best individual survives;
at the end of the search, the set of best individuals forms the final solution.

% Co-evolutionary: COMIX
The MIO approach can be combined with GAs, creating the so-called class of \textit{co-evolutionary} algorithms,
that consists of a set of islands that evolve independently but using standard genetic operators with some periodical migrations (though with some variations).
An example of co-evolutionary algorithm is COMIX (Cooperative Co-evolutionary Algorithm for XMLi)~\cite{jan2019_xmli},
used for the automatic generation of malicious user inputs that exploit XML Injection vulnerabilities in web applications.
An important difference between COMIX and the other aforementioned algorithms is that the goals are not based on
the typical coverage measures from white-box testing, but, instead, they are based on known malicious XML messages, making the strategy totally black-box.

% Evosuite: wholesuite
\citeauthor{fraser2013_evosuite} in the context of \textsc{EvoSuite}~\cite{fraser2013_evosuite} proposed
the \textit{whole suite approach} that changes the point of view:
instead of evolving a population of test cases, it evolves a population of test suites and the single
fitness function models all the coverage goals at once.
The fitness score of a single test suite is an aggregation of the fitness `sub-scores' of each composing test case.
Moreover, the \textsc{EvoSuite}'s algorithm has a preference for smaller test suite, \textit{i.e.,} it tries to find the test suite
with the minimal number of total statements among all individuals.
\textsc{EvoSuite} tool supports multiple coverage criteria, such as branch, method, line, def-use, mutation, exceptions, etc.
and it is able to \textit{generate assertions} (test oracles) for each produced test case of the final test suite, too.
