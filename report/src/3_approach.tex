%! Author = emaia
%! Date = 10/03/20

In this section we explain the novel search algorithm used for the automatic generation of exploit test cases.

\subsection{Context}\label{subsec:assumptions}
% Dataset
We chose to analyze the vulnerabilities of well-known OSS Java libraries and frameworks (\textit{e.g.,} Spring, Jenkins, Tomcat, Kafka, etc.),
publicly disclosed as CVE entries and collected in the NVD. In particular \citeauthor{ponta2019msr_dataset},
defined and made available a manually-curated dataset of patches of these known
vulnerabilities~\cite{ponta2019msr_dataset}.
This dataset maps for each CVE entry the related repository URL (the vast majority is hosted on GitHub) and the hash of the fix commit
(\textit{i.e.,} the commit that officially patches the vulnerability).
So, the dataset has the form of a set of triples \textit{(CVE\_entry, repository\_url, fix\_commit\_hash)}, for example
(CVE-2017-4971, \url{https://github.com/spring-projects/spring-webflow}, 57f2ccb).
The fix commits are very different from each other: some of them apply changes on few lines in a single class,
while others consists of adding, removing and/or modifying entire methods in several classes.
Given a vulnerability, we can obtain the set of vulnerable constructs by looking at which constructs have been impacted
by the changes of the fix commit~\cite{ponta2018icsme_beyond}

% Assumptions
We chose to focus on what we consider the most simple type of vulnerable construct, that is the vulnerable line (\textit{i.e.,}
a source code line that an attacker could exploit by giving it a certain malicious input), so the proposed coverage criterion is a
variant of the line coverage one.
When this approach will be mature enough, we will extend it to other forms of vulnerabilities.

% Evosuite: why extend
We chose to implement this novel algorithm as an extension of \textsc{EvoSuite}~\cite{fraser2013_evosuite},
an automatic \textit{JUnit} test case generator for \textit{Java} classes that uses a whole suite GA to derive
the best possible test suites (\textit{i.e.,} covering all feasible goals and having the minimal number of total statements).
\textsc{EvoSuite} is a fully fledged GA framework, so it allows to (i) add new coverage criteria,
(ii) add new genetic operators (\textit{i.e.,} selection, crossover and mutation), (iii) reuse well-validated meta-heuristics and
(iv) reuse the instrumentation and program analysis tools, such as Control Flow Graphs, Call Graphs and Execution Traces.
It is possible to extend \textsc{EvoSuite} by adding some classes (related to the new coverage criteria and genetic operators),
and make small changes in others.
Sections~\ref{subsec:goals},~\ref{subsec:fitness} and~\ref{subsec:operators} provides additional details on the search algorithm.

In the context of this work we are not generating tests in the narrow sense of the term (\textit{i.e.,} program execution that verifies a certain
expected behaviour) but `exploits cases', so we will disable the assert generation engine.

\subsection{Coverage Goals}\label{subsec:goals}
A coverage goal is defined from a triple of strings \textsc{(vulnerable\_class}, \textsc{vulnerable\_method},
\textsc{vulnerable\_line}) where \textsc{vulnerable\_class} is the fully-qualified name of the vulnerable class (\textit{e.g.,}
\sloppy\textit{hudson.tasks.Mailer}), \textsc{vulnerable\_method} is the name of the vulnerable
method concatenated with its descriptor (\textit{e.g.,}
\sloppy\textit{matches([B[B)Z})
and \textsc{vulnerable\_line} is the number of the vulnerable line in the source code.
The strings \textsc{vulnerable\_class} and \textsc{vulnerable\_method} are used to determine the \textit{call context},
\textit{i.e.,} the list of method calls from the class under test (CUT) needed to statically reach the vulnerable method in the vulnerable class;
the string \textsc{vulnerable\_line} is used to retrieve the list of control dependencies on the vulnerable line
(\textit{i.e.,} the set of branches to be satisfied in order to reach the block of the line).

In other words, each coverage goal is composed of (i) a \textit{target call context} to reach a certain vulnerable method and
(ii) the \textit{target list of control dependencies} necessary to execute the vulnerable line.
An individual (test suite) covers a goal whenever it has a test case whose execution trace covers the call context and all
control dependencies in the list.

\subsection{Fitness Function}\label{subsec:fitness}
The computation of the fitness score of a test case is based on
the \textit{context similarity} and \textit{approach level}:

\begin{definition}
    Given two call contexts $a$ and $t$, the \textit{context similarity} is the ratio
    of the number of method calls that $a$ and $t$ have in common out the total number of the longest call context.
\end{definition}

A test case's execution trace covers various call contexts, so the best context similarity
is the maximum of all context similarities that can be computed against the target call context.

\begin{definition}
    Given an execution trace $e$ and a branch $b$, the \textit{approach level} is the number of
    control dependencies between $b$ and the executed branch that is closest to $b$.
\end{definition}

This definition can be extended to a list of branches (\textit{i.e.,} control dependencies), so the
best approach level is the approach level of the execution trace with respect to the innermost control dependency.

Given a test case's execution trace $e$, the fitness function starts computing the best context similarity,
and if there is a perfect match (\textit{i.e.,} best context similarity is equals to $1$) it proceeds to compute
the best approach level and set its score to the best approach level divided to the total number of target control dependencies;
if there is no perfect match, the fitness score is set to $2 - bestContextSimilarity$; if there no control dependencies at all,
the perfect match is enough to set the score to $0$ (optimum value).
The fitness score $0$ flags the goal as covered.

The fitness scores of a test suite is set to the minimum fitness score among all its test cases;
this implies that a single test case that covered the goal is enough to flag the entire test suite as best individual.
